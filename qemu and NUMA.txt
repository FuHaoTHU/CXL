To have a behavior-level understanding of how CXL devices work as memory expander, we establish a simulation platform based on QEMU, a widely-used simulator for
Computer Architecture simulation. QEMU has built-in CXL devices' model and can support protocols above CXL 2.0, and it treats CXL memory the same as persistent memory
(PMEM). Our system is based on QEMU version 8.0.0 and used an open-sourced Fedora system image file from EEUM's OpenCIS project with Linux kernel version 6.6.8, the CXL system consists of a pxb-CXL bus, a cxl root port, a CXL switch which connects upstream and downstream ports, and a CXL type3 device serves as memory expander. The devices are mapped to Linux kernel using libnvdimm' s ndctl and daxctl tools and finally expose to user space as a dax device "dax0.0". We reconfigured it as an additional system NUMA node and performed basic read-write operation on this device to validate its usability and proved its effectiveness on memory capacity and bandwidth expansion.

To have a real-world device based understanding of CXL memory's performance, we also used remote NUMA memory based emulation, which have been proved to have a similar performance compared to real CXL devices according to previous researches such as ......  . The platform has 2 NUMA nodes with 800 Gigabytes system memory on each, and is equipped with seven NVIDIA Tesla V100 cards with 32GB inside memory for each GPU. We modified pytorch framework to enable its KV cache and KV swap area allocation on fixed remote NUMA nodes' memory rather than traditional thread-bind memory allocate strategy and implanted our new memory-swap and watermark-decision algorithms target for CXL devices. LLM inference models of high memory capacity and bandwidth demands are carried out (like OPT-13B, OPT-66B) to show CXL's unique advantages in unlimited scalability with cheap cost, while other strategies like local-offload or no offload reach their throughput bottlenecks quickly.